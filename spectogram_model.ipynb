{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a data frame with audios"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOME THINGS TO IMPORT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA FRAME FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Give the name of the folder where the audio samples are, if they are in another directory give the complete path\n",
    "\n",
    "def snippet_df_maker(folder):\n",
    "    \"\"\" This function takes as a parameter the path or name to a folder where audio files are saved.\n",
    "        The process made in the function is:\n",
    "        \n",
    "        1. Creates a list with the files names of all the audio snippets. \n",
    "        2. With a for loop, iterates through the files and take the audio waves and the labe(class).\n",
    "        3. It appends each information(iteration into a corresponding list.\n",
    "        4. Creates a dataFrame where the clumns are each of the lists.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store the data\n",
    "    file_names = []\n",
    "    srs = []\n",
    "    audios = []\n",
    "    labels = []\n",
    "    #mfcc = []\n",
    "    \n",
    "    file_list = sorted(file for file in os.listdir(folder) if file.endswith('.wav'))\n",
    "    \n",
    "\n",
    "    \n",
    "    for file in file_list:\n",
    "        \n",
    "        # load the audio file with librosa\n",
    "        audio, sr = librosa.load(os.path.join(folder, file))\n",
    "        audio_norm = librosa.util.normalize(audio)\n",
    "        # split the filename into the label and ID columns\n",
    "        label = file.split('_')[0]#, file.split('_')[1].split('.')[0]\n",
    "        #mfccs = librosa.feature.mfcc(y=y, sr=sr)[0:13]\n",
    "        \n",
    "        file_names.append(file)\n",
    "        srs.append(sr)\n",
    "        audios.append(audio_norm)\n",
    "        labels.append(label)\n",
    "        #mfcc.append(mfccs)\n",
    "\n",
    "        # convert the lists to a pandas dataframe\n",
    "        snippet_df = pd.DataFrame(\n",
    "            {'file_name':file_names,\n",
    "            'sample_rate': srs,\n",
    "            'audio': audios,\n",
    "            'label': labels})\n",
    "            #\"mfccs\": mfcc})\n",
    "        \n",
    "        # add length column\n",
    "        snippet_df['length'] = (snippet_df['audio'].apply(lambda x: len(x))/snippet_df['sample_rate'])\n",
    "        \n",
    "        # include only rows where length between 0.2 and 10 seconds\n",
    "        snippet_df = snippet_df[snippet_df['length'] > 0.2]\n",
    "        snippet_df = snippet_df[snippet_df['length'] < 4]\n",
    "         \n",
    "    return snippet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataFrame\n",
    "folder = \"clean_data/snippet_samples\" \n",
    "df = snippet_df_maker(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bark_00001.wav</td>\n",
       "      <td>22050</td>\n",
       "      <td>[0.023036616, 0.023904754, 0.02477289, 0.02347...</td>\n",
       "      <td>bark</td>\n",
       "      <td>0.695964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bark_00005.wav</td>\n",
       "      <td>22050</td>\n",
       "      <td>[0.0009460449, 0.001159668, 0.00091552734, 0.0...</td>\n",
       "      <td>bark</td>\n",
       "      <td>1.088027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name  sample_rate  \\\n",
       "0  bark_00001.wav        22050   \n",
       "1  bark_00005.wav        22050   \n",
       "\n",
       "                                               audio label    length  \n",
       "0  [0.023036616, 0.023904754, 0.02477289, 0.02347...  bark  0.695964  \n",
       "1  [0.0009460449, 0.001159668, 0.00091552734, 0.0...  bark  1.088027  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "le = LabelEncoder()\n",
    "df[\"num_class\"] = le.fit_transform(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bark', 'growl', 'pant', 'whine'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>sample_rate</th>\n",
       "      <th>audio</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>num_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bark_00001.wav</td>\n",
       "      <td>22050</td>\n",
       "      <td>[0.023036616, 0.023904754, 0.02477289, 0.02347...</td>\n",
       "      <td>bark</td>\n",
       "      <td>0.695964</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bark_00005.wav</td>\n",
       "      <td>22050</td>\n",
       "      <td>[0.0009460449, 0.001159668, 0.00091552734, 0.0...</td>\n",
       "      <td>bark</td>\n",
       "      <td>1.088027</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file_name  sample_rate  \\\n",
       "0  bark_00001.wav        22050   \n",
       "1  bark_00005.wav        22050   \n",
       "\n",
       "                                               audio label    length  \\\n",
       "0  [0.023036616, 0.023904754, 0.02477289, 0.02347...  bark  0.695964   \n",
       "1  [0.0009460449, 0.001159668, 0.00091552734, 0.0...  bark  1.088027   \n",
       "\n",
       "   num_class  \n",
       "0          0  \n",
       "1          0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    27.404516\n",
       "3    27.097853\n",
       "2    23.919710\n",
       "0    21.577920\n",
       "Name: num_class, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.num_class.value_counts()/df.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    983\n",
       "3    972\n",
       "2    858\n",
       "0    774\n",
       "Name: num_class, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.num_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "growl    27.404516\n",
       "whine    27.097853\n",
       "pant     23.919710\n",
       "bark     21.577920\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.label.value_counts()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "growl    983\n",
       "whine    972\n",
       "pant     858\n",
       "bark     774\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resampled, df_resampled.label = ros.fit_resample(df, df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bark     25.0\n",
       "growl    25.0\n",
       "pant     25.0\n",
       "whine    25.0\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_resampled.label.value_counts()/df_resampled.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bark     983\n",
       "growl    983\n",
       "pant     983\n",
       "whine    983\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled.label.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the spectograms "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THINGS TO IMPORT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the name/ path where the audio data is and the folder where the spectrograms will be saved. \n",
    "audio_data_directory = \"clean_data/snippet_samples\"\n",
    "spectrograms_directory = \"clean_data/spectogram_samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_spectrograms(row):\n",
    "    \"\"\"\n",
    "    IMPORTANT: CHECK COLUMNS NAMES\n",
    "    This function takes as parameters a row of the data frame and creates the spectogram for each audio, \n",
    "    saving them as .png in new folders based on the class where they belong.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    audio_class = row[\"num_class\"]\n",
    "    spect_directory = \"class_\" + str(audio_class)\n",
    "    audio_file_name_without_extension = row[\"file_name\"][:-4]\n",
    "    \n",
    "    y = row[\"audio\"]\n",
    "    \n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    librosa.display.specshow(spectrogram, y_axis='linear')\n",
    "    \n",
    "    plt.savefig(spectrograms_directory + \"/\" + spect_directory + \"/\" + audio_file_name_without_extension + \".png\")\n",
    "\n",
    "    plt.clf() \n",
    "    plt.close('all')\n",
    "    gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE THE FOLDERS FOR EACH CLASS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This creates folder in the spectograms directory to divide the new images depending on the classes. \n",
    "for i in range(0,4):\n",
    "    Path(spectrograms_directory + \"/\" + \"class_\" + str(i)).mkdir(parents=True, exist_ok=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "3927    None\n",
       "3928    None\n",
       "3929    None\n",
       "3930    None\n",
       "3931    None\n",
       "Length: 3932, dtype: object"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resampled.apply(generate_spectrograms, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27.404516\n",
       "1    27.404516\n",
       "2    27.404516\n",
       "3    27.404516\n",
       "Name: num_class, dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df_resampled.num_class.value_counts()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3587 files belonging to 4 classes.\n",
      "Using 2870 files for training.\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 12:27:13.324268: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-14 12:27:13.324619: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3587 files belonging to 4 classes.\n",
      "Using 717 files for validation.\n"
     ]
    }
   ],
   "source": [
    "X_train = image_dataset_from_directory(spectrograms_directory,validation_split = 0.2,\n",
    "                                                              subset = \"training\", seed=7)\n",
    "X_test = image_dataset_from_directory(spectrograms_directory,validation_split = 0.2,\n",
    "                                                             subset=\"validation\", seed=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, losses, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Reshape((256, 256, 3), input_shape=(256, 256, 3)))\n",
    "model.add(layers.experimental.preprocessing.Rescaling(1.0/255.0))\n",
    "\n",
    "model.add(layers.Conv2D(32, 7, strides = 4, padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "model.add(layers.MaxPooling2D((4,4)))\n",
    "model.add(layers.Conv2D(128, 3, padding=\"same\"))\n",
    "model.add(layers.BatchNormalization())\n",
    "\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Activation(\"relu\"))\n",
    "\n",
    "model.add(layers.Dense(5, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= \"adam\", loss=losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience= 40, restore_best_weights= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 12:27:14.665204: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-14 12:27:14.667287: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.4049 - accuracy: 0.8582"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 12:27:24.948830: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 12s 121ms/step - loss: 0.4049 - accuracy: 0.8582 - val_loss: 1.9914 - val_accuracy: 0.2483\n",
      "Epoch 2/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.1972 - accuracy: 0.9279 - val_loss: 3.2649 - val_accuracy: 0.2497\n",
      "Epoch 3/500\n",
      "90/90 [==============================] - 9s 92ms/step - loss: 0.1571 - accuracy: 0.9401 - val_loss: 2.0649 - val_accuracy: 0.4519\n",
      "Epoch 4/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.1128 - accuracy: 0.9582 - val_loss: 0.6230 - val_accuracy: 0.7852\n",
      "Epoch 5/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0886 - accuracy: 0.9707 - val_loss: 0.6493 - val_accuracy: 0.8145\n",
      "Epoch 6/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.0793 - accuracy: 0.9707 - val_loss: 0.3631 - val_accuracy: 0.8717\n",
      "Epoch 7/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0502 - accuracy: 0.9836 - val_loss: 1.9285 - val_accuracy: 0.5997\n",
      "Epoch 8/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0511 - accuracy: 0.9808 - val_loss: 3.6560 - val_accuracy: 0.5425\n",
      "Epoch 9/500\n",
      "90/90 [==============================] - 9s 96ms/step - loss: 0.0313 - accuracy: 0.9899 - val_loss: 1.6878 - val_accuracy: 0.6499\n",
      "Epoch 10/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0294 - accuracy: 0.9909 - val_loss: 1.2194 - val_accuracy: 0.7197\n",
      "Epoch 11/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0364 - accuracy: 0.9899 - val_loss: 0.3993 - val_accuracy: 0.8438\n",
      "Epoch 12/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0421 - accuracy: 0.9857 - val_loss: 1.4601 - val_accuracy: 0.6987\n",
      "Epoch 13/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0154 - accuracy: 0.9965 - val_loss: 0.1475 - val_accuracy: 0.9470\n",
      "Epoch 14/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0141 - accuracy: 0.9965 - val_loss: 0.3533 - val_accuracy: 0.8884\n",
      "Epoch 15/500\n",
      "90/90 [==============================] - 9s 102ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.4617 - val_accuracy: 0.8815\n",
      "Epoch 16/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0175 - accuracy: 0.9948 - val_loss: 3.4197 - val_accuracy: 0.4198\n",
      "Epoch 17/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0173 - accuracy: 0.9955 - val_loss: 0.5644 - val_accuracy: 0.8187\n",
      "Epoch 18/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0158 - accuracy: 0.9941 - val_loss: 0.2108 - val_accuracy: 0.9331\n",
      "Epoch 19/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0144 - accuracy: 0.9965 - val_loss: 0.7435 - val_accuracy: 0.8494\n",
      "Epoch 20/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.3910 - val_accuracy: 0.8647\n",
      "Epoch 21/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0305 - accuracy: 0.9895 - val_loss: 3.1403 - val_accuracy: 0.5425\n",
      "Epoch 22/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0437 - accuracy: 0.9868 - val_loss: 4.4107 - val_accuracy: 0.5481\n",
      "Epoch 23/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0278 - accuracy: 0.9899 - val_loss: 0.2769 - val_accuracy: 0.9205\n",
      "Epoch 24/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0150 - accuracy: 0.9958 - val_loss: 0.3113 - val_accuracy: 0.9010\n",
      "Epoch 25/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.0063 - accuracy: 0.9986 - val_loss: 0.4455 - val_accuracy: 0.8787\n",
      "Epoch 26/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.7850 - val_accuracy: 0.8173\n",
      "Epoch 27/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.5427 - val_accuracy: 0.8298\n",
      "Epoch 28/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.2170 - val_accuracy: 0.9498\n",
      "Epoch 29/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.2176 - val_accuracy: 0.9372\n",
      "Epoch 30/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0079 - accuracy: 0.9969 - val_loss: 0.4736 - val_accuracy: 0.8759\n",
      "Epoch 31/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 2.0642 - val_accuracy: 0.7183\n",
      "Epoch 32/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0038 - accuracy: 0.9990 - val_loss: 0.5482 - val_accuracy: 0.8787\n",
      "Epoch 33/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.2802 - val_accuracy: 0.9247\n",
      "Epoch 34/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1745 - val_accuracy: 0.9609\n",
      "Epoch 35/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 7.0936e-04 - accuracy: 1.0000 - val_loss: 0.2144 - val_accuracy: 0.9526\n",
      "Epoch 36/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.2108 - val_accuracy: 0.9582\n",
      "Epoch 37/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 5.1934 - val_accuracy: 0.5718\n",
      "Epoch 38/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0809 - accuracy: 0.9714 - val_loss: 21.7019 - val_accuracy: 0.2092\n",
      "Epoch 39/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.0564 - accuracy: 0.9808 - val_loss: 1.2923 - val_accuracy: 0.7141\n",
      "Epoch 40/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0320 - accuracy: 0.9885 - val_loss: 3.8304 - val_accuracy: 0.6834\n",
      "Epoch 41/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.3933 - val_accuracy: 0.8675\n",
      "Epoch 42/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0116 - accuracy: 0.9969 - val_loss: 0.4475 - val_accuracy: 0.8968\n",
      "Epoch 43/500\n",
      "90/90 [==============================] - 9s 102ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.2573 - val_accuracy: 0.9121\n",
      "Epoch 44/500\n",
      "90/90 [==============================] - 10s 102ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.4005 - val_accuracy: 0.9066\n",
      "Epoch 45/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0037 - accuracy: 0.9993 - val_loss: 0.3507 - val_accuracy: 0.9261\n",
      "Epoch 46/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.2914 - val_accuracy: 0.9414\n",
      "Epoch 47/500\n",
      "90/90 [==============================] - 9s 100ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1988 - val_accuracy: 0.9568\n",
      "Epoch 48/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.2658 - val_accuracy: 0.9247\n",
      "Epoch 49/500\n",
      "90/90 [==============================] - 9s 97ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.2033 - val_accuracy: 0.9512\n",
      "Epoch 50/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0017 - accuracy: 0.9993 - val_loss: 0.3142 - val_accuracy: 0.9205\n",
      "Epoch 51/500\n",
      "90/90 [==============================] - 9s 98ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.2049 - val_accuracy: 0.9414\n",
      "Epoch 52/500\n",
      "90/90 [==============================] - 9s 99ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.3338 - val_accuracy: 0.9121\n",
      "Epoch 53/500\n",
      "90/90 [==============================] - 9s 101ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2853 - val_accuracy: 0.9470\n"
     ]
    }
   ],
   "source": [
    "model_history = model.fit(X_train, validation_data=X_test, epochs=500, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last loss is 0.0016290458152070642\n",
      "Last acuracy is 0.9996516108512878\n",
      "Last validation loss is 0.2852921187877655\n",
      "Last validation acuracy is 0.9470013976097107\n"
     ]
    }
   ],
   "source": [
    "print (f'Last loss is {model_history.__dict__[\"history\"][\"loss\"][-1]}')\n",
    "print (f'Last acuracy is {model_history.__dict__[\"history\"][\"accuracy\"][-1]}')\n",
    "print (f'Last validation loss is {model_history.__dict__[\"history\"][\"val_loss\"][-1]}')\n",
    "print (f'Last validation acuracy is {model_history.__dict__[\"history\"][\"val_accuracy\"][-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt_loss = plt.subplot(121)\n",
    "plt_loss.plot(model_history.history[\"loss\"])\n",
    "plt_loss.plot(model_history.history[\"val_loss\"])\n",
    "\n",
    "# plt.title(\"\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"upper right\")\n",
    "\n",
    "plt_accuracy = plt.subplot(122)\n",
    "plt_accuracy.plot(model_history.history[\"accuracy\"])\n",
    "plt_accuracy.plot(model_history.history[\"val_accuracy\"])\n",
    "# plt.title(\"\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Training\", \"Validation\"], loc=\"lower right\")\n",
    "plt.ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the data\n",
    "file_names = []\n",
    "audios = []\n",
    "labels = []\n",
    " \n",
    "file_list = sorted(os.listdir(\"clean_data/test_1\"))\n",
    "file_list.remove(\".DS_Store\")\n",
    "\n",
    "    \n",
    "for file in file_list:\n",
    "        \n",
    "    # load the audio file with librosa\n",
    "    audio, sr = librosa.load(os.path.join(\"clean_data/test_1\", file))\n",
    "    audio_norm = librosa.util.normalize(audio)\n",
    "    # split the filename into the label and ID columns\n",
    "    label = file.split('_')[0]#, file.split('_')[1].split('.')[0]\n",
    "        \n",
    "        \n",
    "    file_names.append(file)\n",
    "    audios.append(audio_norm)\n",
    "    labels.append(label)\n",
    "     \n",
    "\n",
    "    # convert the lists to a pandas dataframe\n",
    "    test_df = pd.DataFrame(\n",
    "            {'file_name':file_names,\n",
    "            'audio': audios,\n",
    "            'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_spect(row):\n",
    "\n",
    "    spect_directory = \"clean_data/spectogram_test_1\"\n",
    "    audio_file_name_without_extension = row[\"file_name\"][:-4]\n",
    "    \n",
    "    y = row[\"audio\"]\n",
    "    \n",
    "    spectrogram = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    librosa.display.specshow(spectrogram, y_axis='linear')\n",
    "    \n",
    "    plt.savefig(spect_directory + \"/\" + audio_file_name_without_extension + \".png\")\n",
    "\n",
    "    plt.clf() \n",
    "    plt.close('all')\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.apply(new_spect, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction (row):\n",
    "    audio_file_name_without_extension = row[\"file_name\"][:-4]\n",
    "    X_test = np.array(load_img(f'clean_data/spectogram_test_1/{audio_file_name_without_extension}.png'))\n",
    "    resized_img = np.array(Image.fromarray(X_test).resize((256, 256)))\n",
    "    row[\"prediction\"] = model.predict(np.expand_dims(resized_img,axis=0))\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pred = test_df.apply(prediction, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_pred (row):\n",
    "    prediction = row[\"prediction\"]\n",
    "    print (f'name = {row[\"file_name\"]}')\n",
    "    print (f'real = {row[\"label\"]}')\n",
    "    print(f'bark = {list(prediction[0])[0]*100}')\n",
    "    print(f'growl = {list(prediction[0])[1]*100}')\n",
    "    print(f'pant = {list(prediction[0])[2]*100}')\n",
    "    print(f'whine = {list(prediction[0])[3]*100}')\n",
    "    print(\"_____________________________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_pred.apply(result_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test youtube\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the data\n",
    "file_names = []\n",
    "audios = []\n",
    "labels = []\n",
    " \n",
    "file_list = sorted(os.listdir(\"clean_data/youtube\"))\n",
    "#file_list.remove(\".DS_Store\")\n",
    "\n",
    "    \n",
    "for file in file_list:\n",
    "        \n",
    "    # load the audio file with librosa\n",
    "    audio, sr = librosa.load(os.path.join(\"clean_data/youtube\", file))\n",
    "    audio_norm = librosa.util.normalize(audio)\n",
    "    # split the filename into the label and ID columns\n",
    "    label = file.split('_')[0]#, file.split('_')[1].split('.')[0]\n",
    "        \n",
    "        \n",
    "    file_names.append(file)\n",
    "    audios.append(audio_norm)\n",
    "    labels.append(label)\n",
    "     \n",
    "\n",
    "    # convert the lists to a pandas dataframe\n",
    "    test_df = pd.DataFrame(\n",
    "            {'file_name':file_names,\n",
    "            'audio': audios,\n",
    "            'label': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.apply(new_spect, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laica",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
